{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "#load data\n",
    "mnist_train = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_test = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# make validation set\n",
    "\n",
    "train_length = len(mnist_train)\n",
    "train_size = int(0.85 * train_length)\n",
    "val_size = train_length - train_size\n",
    "\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [train_size, val_size])\n",
    "\n",
    "# store train and val loss\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "epoch_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x28834df65e0>"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise sample of data to know what we are dealing with\n",
    "plt.figure(1)\n",
    "img, label = mnist_train[5]\n",
    "plt.imshow(img.squeeze(), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "# make network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128) # 16 from batchnorm, 14 from pooling\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        # self.act2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(-1, 64 * 7 * 7)\n",
    "        out = self.fc1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.act2(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# train and test loop\n",
    "def train_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader):\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        train_total = 0.0\n",
    "        train_correct = 0.0\n",
    "        val_total = 0.0\n",
    "        val_correct = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            # move tensors to gpu\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1) # Gives us the index of the highest value\n",
    "            train_total += labels.shape[0]  # Counts the number of examples, so total is increased by the batch size\n",
    "            train_correct += int((predicted == labels).sum())\n",
    "\n",
    "\n",
    "\n",
    "        epoch_list.append(epoch)\n",
    "        train_loss_list.append(loss_train / len(train_loader))\n",
    "\n",
    "        # get loss of validation data\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.0\n",
    "\n",
    "\n",
    "            for imgs, labels in val_loader:\n",
    "                # move tensors to gpu if available\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                loss_v = loss_fn(outputs, labels)\n",
    "\n",
    "                loss_val += loss_v.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1) # Gives us the index of the highest value\n",
    "                val_total += labels.shape[0]  # Counts the number of examples, so total is increased by the batch size\n",
    "                val_correct += int((predicted == labels).sum())\n",
    "\n",
    "\n",
    "\n",
    "        val_loss_list.append(loss_val / len(val_loader))\n",
    "\n",
    "        val_acc = val_correct/val_total\n",
    "\n",
    "        train_acc_list.append(train_correct/train_total)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            print('Found better model')\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'mnist_digits_nn.pt')\n",
    "\n",
    "        # set when to print info about training progress\n",
    "        if epoch == 1 or epoch % 1 == 0:\n",
    "            print('Epoch {}, Training loss {:.4f}, Validation loss {:.4f}, Train Acc {:.4f}, Val Acc {:.4f}'.format(epoch, loss_train / len(train_loader), loss_val / len(val_loader), train_correct/train_total, val_acc),\n",
    "                  )\n",
    "            print('Best Val Acc {:.4f}'.format(best_val_acc))\n",
    "        \n",
    "\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('mnist_digits_nn.pt'))\n",
    "\n",
    "\n",
    "def test_loop(model, test_loader):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            # move to gpu\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1) # Gives us the index of the highest value\n",
    "            total += labels.shape[0]  # Counts the number of examples, so total is increased by the batch size\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "\n",
    "    print(\"Accuracy test: {:.4f} %\".format(correct / total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found better model\n",
      "Epoch 1, Training loss 0.5013, Validation loss 0.3639, Train Acc 0.8385, Val Acc 0.8721\n",
      "Best Val Acc 0.8721\n",
      "Found better model\n",
      "Epoch 2, Training loss 0.2890, Validation loss 0.2889, Train Acc 0.8943, Val Acc 0.8982\n",
      "Best Val Acc 0.8982\n",
      "Found better model\n",
      "Epoch 3, Training loss 0.2490, Validation loss 0.2717, Train Acc 0.9100, Val Acc 0.9038\n",
      "Best Val Acc 0.9038\n",
      "Found better model\n",
      "Epoch 4, Training loss 0.2248, Validation loss 0.2752, Train Acc 0.9171, Val Acc 0.9042\n",
      "Best Val Acc 0.9042\n",
      "Epoch 5, Training loss 0.2056, Validation loss 0.3032, Train Acc 0.9262, Val Acc 0.9007\n",
      "Best Val Acc 0.9042\n",
      "Epoch 6, Training loss 0.1931, Validation loss 0.3203, Train Acc 0.9294, Val Acc 0.9000\n",
      "Best Val Acc 0.9042\n",
      "Epoch 7, Training loss 0.1842, Validation loss 0.3527, Train Acc 0.9322, Val Acc 0.8890\n",
      "Best Val Acc 0.9042\n",
      "Epoch 8, Training loss 0.1753, Validation loss 0.3215, Train Acc 0.9362, Val Acc 0.9017\n",
      "Best Val Acc 0.9042\n",
      "Epoch 9, Training loss 0.1626, Validation loss 0.3186, Train Acc 0.9400, Val Acc 0.9026\n",
      "Best Val Acc 0.9042\n",
      "Epoch 10, Training loss 0.1568, Validation loss 0.3549, Train Acc 0.9423, Val Acc 0.9002\n",
      "Best Val Acc 0.9042\n",
      "Epoch 11, Training loss 0.1493, Validation loss 0.3570, Train Acc 0.9445, Val Acc 0.8979\n",
      "Best Val Acc 0.9042\n",
      "Epoch 12, Training loss 0.1422, Validation loss 0.3410, Train Acc 0.9476, Val Acc 0.9038\n",
      "Best Val Acc 0.9042\n",
      "Epoch 13, Training loss 0.1344, Validation loss 0.3858, Train Acc 0.9512, Val Acc 0.9003\n",
      "Best Val Acc 0.9042\n"
     ]
    }
   ],
   "source": [
    "# set model and params\n",
    "n_epochs = 25\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(mnist_val, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test,  batch_size=64, shuffle=False)\n",
    "# epoch_num_of_no_improve = 5\n",
    "\n",
    "\n",
    "train_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader)\n",
    "\n",
    "\n",
    "print('Accuracy score')\n",
    "test_loop(model, val_loader)\n",
    "print('Test score')\n",
    "test_loop(model, test_loader)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.plot(epoch_list, train_loss_list, color='blue', label='train_loss')\n",
    "plt.plot(epoch_list, val_loss_list, color='green', label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.plot(epoch_list, train_acc_list, color='blue', label='train acc')\n",
    "plt.plot(epoch_list, val_acc_list, color='green', label='validation acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "y_fit = np.array([])\n",
    "y_test = np.array([])\n",
    "\n",
    "for imgs, labels in test_loader:\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    outputs = model(imgs)\n",
    "\n",
    "    _, predicted = torch.max(outputs, dim=1) # Gives us the index of the highest value\n",
    "    predicted = predicted.detach().numpy()\n",
    "    labels = labels.detach().numpy()\n",
    "    y_fit = np.concatenate((y_fit, predicted), axis=None)\n",
    "    y_test = np.concatenate((y_test, labels), axis=None)\n",
    "    # y_fit.append(predicted)\n",
    "    # y_test.append(labels)\n",
    "\n",
    "labels_names = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
    "\n",
    "plt.figure(6, dpi=180)\n",
    "conf_matrix = confusion_matrix(y_test, y_fit)\n",
    "conf_matrix = pd.DataFrame(conf_matrix)\n",
    "sns.heatmap(conf_matrix.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=labels_names, yticklabels=labels_names, cmap='magma')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}